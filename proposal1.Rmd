---
title: 'Project Proposal'
author: 'Isaac Cheong, Sam Tan, Max Zhang'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

load('ICPSR_20520/DS0001/20520-0001-Data.rda')  # creates df called da20520.0001
cils <- da20520.0001
```

# Potential title: 
- Social mobility of the children of immigrants in America
- The Adaptation of the Immigrant Second Generation in America

# Introduction

What makes some second generation Americans more adaptive to the mainstrain society than others? In this report, we set out to investiage this question by predicting one of the important measurements for overall happiness: income and financial satisfication. Using survey data provided by the Center for Migration and Development at princeton univeristy, we will predict the income level of 2500+ second generation Americans residing in San Diego and Miami in 2005 using baseline information on immigrant families and childrenâ€™s own demographic characteristics: language use; self-identities; and academic attainment. 

# Data Description


# EDA

# Model Selection
There are 3 major methods for selecting models: shrinkage(ridge and LASSO), greedy selection (stepwise/forward/backward selection) and optional(exhaustive) subset selection. Since we have roughly around 400 categorical variables (each with roughly 4 values), we will have around 1600 individual dummy variables. For exhaustive subset selection, it will take way too long to run. For greedy selection, it will only explore a small number of possible models relative to all the possible model options (2^1600) out there. Shrinkage methods like LASSO will have good performance for models with many possible features like ours because it remains effective regardless of the number of explanatory variables. In particular, we will use LASSO to make predictions because it will provide sparser coefficients, which allows us to use those remaining variables to do inference in later sections. 

To do LASSO, we will first standardize the variable, so larger weights reflects greater importance. By the time we shrink the variables, only the important ones will be left. We will choose the penality size lambda based on cross validation(#?). 

# Model Diagnostics
There are 4 major things to check for diagnosting a model. Because we don't know what the real epsilons are, we will approximate them by using the fitted residuals(#?). 

To check whether the mean of the epsilons is centered at 0, we will plot residuals against all the exploratory variables and do feature engineering if necessary to transform specific variables that don't have a zero mean(#?). To check whether the epsilons have constant variance, we will plot fitted value of y against the residuals. If larger values of y conrrespond to a large variance of residuals, we will do bootstrap by cases during inference. To check normality for our residuals, we will make a normal Q-Q plot of standardized residuals plotted against our theoretical quantiles, checking if the points line up across the y = x line. If it's not, then we will reply on bootstraps by cases to do inference. 


# Conclusion

\newpage

# Appendix

```{r}
head(cils[ ,1:5], 5)
```




